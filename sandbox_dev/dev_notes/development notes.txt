Each investment period is a node. The first investment period will be the root node of the scenario tree. It will have one child node for the next investment period and multiple child nodes for dispatch scenarios that could occur within that period. 

In the scenario tree, I need to be able to write stuff like:
set StageVariables[Invest_Stage1] := BuildProjects[Period1,*] RetireExistingPlant[Period1,*] BuildTrans[Period1, *] ...
set StageVariables[Dispatch_Stage1a] := DispatchGen[Scenario1a,*,*]  DispatchTrans[Scenario1a,*,*]  DispatchStorage[Scenario1a,*,*] ...
This duplicates information in the indexes since each timepoint is uniquely associated with a single period, but adding the period indexes will enable much simpler specification of problem stages. I'm currently thinking that stages will progress: Invest1, Dispatch1a, Dispatch1b, Invest2, Dispatch2a, Dispatch2b, ..., Invest-n, Dispatch-n

2/17 PM & 2/18 AM. 
I'm trying to structure and document the code as I go for maximal readability. I still haven't settled on a package/module structure and I will most likely reorganize as I proceed and learn more about python. Right now, I'm aiming for different files to contain python functions that add define major chunks of the switch model. timescales.py is the first of these files. 
I looked into python autodocumentation packages and started using pydoc to get started. I think Sphinx would be better eventually, but that has a bit more set-up and learning to it. Hopefully it won't be too hard to transition text from basic pydoc format (multi-line comments at the beginning of files, functions or classes) to whatever Sphinx prefers. As part of this autodoc effort, I moved most of the inline comments to the multi-line comments. That actually makes the code cleaner and easier to read, in my opinion. 
As I was trying to figure out if pydoc supported any simple markup (it doesn't), I ran into doctest. It's a simple framework that will look for any example code snippets and expected output in the multi-line comments (same ones that pydoc examines). Whatever it finds, it executes and compares literal output to expected output. It's clean and simple, but not terribly scalable. Unittest is the scalable version, but that has a little more learning curve to it, so I'm punting on that for the moment. doctest can integrate with unittest, so whatever simple tests I do with doctest can be re-used later. Anyway, to take advantage of doctest, I merged the contents of the test_timescales() with the SYNOPSIS section. I wrote two more data files that should fail in different ways, but those aren't as appropriate for doctest because part of the error messages include library paths that are specific to my system. Oh well. 

2/28
I rewrote chunks of timescales.py last week, shortening parameter & set names, making them more consistent, moving timepoint weight specifications to timeseries to avoid potential for confusion.. I wrote some notes into a git commit. 

3/2
Pyomo has a reputation for being tricky to debug. A mistake or missing datum can cause funky error messages that don't tell you directly what's going on. Adding validation checks via BuildCheck() can relieve some of this headache, I think. I figured out a way of using python introspection to validate that an arbitrary list of mandatory parameters has been specified. It seems overkill for base financial info where I wrote it, because the list only has two members. Still, it's a useful bit of code to retain, so I'm pasting it here. 
	def validate_minimum_financial_data_rule (mod):
		mandatory_params = set(['base_financial_year', 'interest_rate'])
		for active_param in mod.active_components(Param):
			if ( active_param in mandatory_params ):
				obj = getattr(mod, active_param)
				if ( obj() is None ):
					print "Mandatory parameter " + active_param + " is missing!"
					return 0
		return 1
	switch.minimum_data_requirements = BuildCheck(rule=validate_minimum_financial_data_rule)

3/4
I tried using .dat file load commands to load .tab files because I thought this might be needed for some PySP scenario tree stuff. 
See https://software.sandia.gov/downloads/pub/pyomo/PyomoOnlineDocs.html#_data_command_files
I could only get it to work when the .tab files were in the current working directory. Specifying a path such as test_dat/dispatch_scenarios.tab instead of dispatch_scenarios.tab generated an error, complaining that / is an illegal character. Wrapping the path in quotes generated an even more obscure error message during model instantiation. The documentation didn't help at all with this. I read the code that parses these commands (pyomo/core/data/parse_datacmds.py, ampl.py, proess_data.py), but couldn't find a way of specifying a directory name.
Anyway, the following commands work when the .tab files are in the working directory. 
	load periods.tab : PERIODS = [PERIOD] period_start period_end;
	load dispatch_scenarios.tab : DISPATCH_SCENARIOS = [DISPATCH_SCENARIO] disp_scen_period=period disp_scen_dbid=dbid;
	load timeseries.tab : TIMESERIES=[TIMESERIES] ts_disp_scen ts_duration_of_tp ts_num_tps ts_scale_to_period;
	load timepoints.tab : TIMEPOINTS=[timepoint_id] tp_timestamp=timepoint_label tp_ts=timeseries;

3/6
Today I pushed my boundaries of using DataPortal: loading parameters with multi-dimensional indexing. I encountered numerous problems that ultimately were rooted in the data file that I thought I was using really being in a different directory. Sublime doesn't update itself like BBEdit if an open file is moved to a new directory in the background. 
I learned that DataPortal.load() is sparsely documented and doesn't appear at all in the Pyomo book, even though the website documentation for DataPortal references the book. https://software.sandia.gov/downloads/pub/pyomo/PyomoOnlineDocs.html#_dataportal_objects
DataPortal seems to have suplanted the ModelData class. The load function in the DataPortal object has a bunch of TODO's in place of desscriptive text in its inline documentation. 
I learned that if you want to select just one column from a text .tab file with load(), you need to wrap the column name in a tuple (not a list), or else the argument processing code in TableData.py will break it down character-by-character. So this works
    switch_data.load(
        filename=os.path.join(inputs_dir, 'load_zones.tab'),
        select=['balancing_area'],
        param=mod.foo)
But select='balancing_area' and select=('balancing_area') both generate an error:
"ValueError: 'b' is not in list", meaning the list of header columns in the .tab file. 
You cannot build a set from a single column of a multi-column file. If you read a file into a set, the parser will ignore the select clause, will assume the set is multi-dimensional, cast the entire row as a tuple, try to append that tuple to the set, and generate an error message when the set complains about a dimensionality mis-match. 

To print debugging messages in my code or the Pyomo code, this code injection works well:
	import logging
	logging.warning('obj value is')
	logging.warning(obj)

To deal with balancing areas being optional, I ended up initializing them to the unique values in the balancing_area column of load_zones.tab. Then I read in from the balancing_areas.tab file if it exists. If I set index=mod.BALANCING_AREAS, then this file overrides the initial value. Interesting. One upshot is that I still need to check that balancing area names match between load zone data and balancing area data if I specify the index. I can also leave the index clause out completely if I specify the index as the first column in the select clause like so:
    switch_data.load(
        filename=balancing_area_path,
        select=(
            'BALANCING_AREAS', 'quickstart_res_load_frac',
            'quickstart_res_wind_frac', 'quickstart_res_solar_frac',
            'spinning_res_load_frac', 'spinning_res_wind_frac',
            'spinning_res_solar_frac'),
        #index=mod.BALANCING_AREAS,
        param=(mod.quickstart_res_load_frac, mod.quickstart_res_wind_frac,
               mod.quickstart_res_solar_frac, mod.spinning_res_load_frac,
               mod.spinning_res_wind_frac, mod.spinning_res_solar_frac))
This formulation actually generates a reasonable error message if there is a balancing area in this file that doesn't match the pre-existing set. For example, I have NorthCentral and South defined in load_zones.tab. If I specify South_typo in balancing_areas.tab, I get the error:
    RuntimeError: Failed to set value for param=quickstart_res_load_frac, index=South_typo, value=0.04.
    	source error message="Error setting parameter value: Index 'South_typo' is not valid for array Param 'quickstart_res_load_frac'"

3/18/2015
The implementation of the fuels module went slower than I liked. I did manage to merge all of the custom hacks into a single uniform implementation that relied on regional fuel markets with supply curves with optional load-zone level flat cost adjustments. To support NG price elasticity, biogas limits and biomass supply curves we need some code for supply curves and regional price adjustments, and the code complexity doesn't increase if that code supports that functionality for all fuels. Still, this module ended up a bit messier than others.

I wanted to support an optional input file that specified simple costs for fuels per load zone and period without upper limits on consumption, and I learned how to manipulate Pyomo's DataPortal() object to accomplish this. Basically, it stores everything it reads in in a nested dictionary. Accessing one of the data components (sets or parameters at least) for reads or writes is just a matter of using the data() function. A read example from the switch_data object: switch_data.data(name='FUELS'). In theory, I could also access it with switch_data.data()['FUELS'], but this returns a dictionary of FUELS indexed by namespaces, with a default namespace of 'None':
>>> switch_data.data()['FUELS']
{None: ['Coal', 'ResidualFuelOil', 'DistillateFuelOil', 'NaturalGas', 'Uranium', 'BioSolid']}
This namespace indexing is not consistent between sets and parameters, as you can see with the example from the parameter f_co2_intensity:
>>> switch_data.data()['f_co2_intensity']
{'ResidualFuelOil': 0.0788, 'Uranium': 0, 'DistillateFuelOil': 0.07315, 'Coal': 0.09552, 'NaturalGas': 0.05306, 'BioSolid': 0.09435}
I can get uniform behavior of avoiding namespace indexing if I use switch_data.data(name='FUELS') and switch_data.data(name='f_co2_intensity'), so that is my preferred usage:
>>> switch_data.data(name='FUELS')
['Coal', 'ResidualFuelOil', 'DistillateFuelOil', 'NaturalGas', 'Uranium', 'BioSolid']
>>> switch_data.data(name='f_co2_intensity')
{'ResidualFuelOil': 0.0788, 'Uranium': 0, 'DistillateFuelOil': 0.07315, 'Coal': 0.09552, 'NaturalGas': 0.05306, 'BioSolid': 0.09435}
You can directly manipulate this objects, adding new elements or overwriting components, and that data will be used when a model instance is instantiated. 
>>> switch_data.data(name='FUELS').append('SolidWaste')
>>> switch_data.data(name='f_co2_intensity')['SolidWaste'] = 0.095
>>> switch_data.data(name='FUELS')
['Coal', 'ResidualFuelOil', 'DistillateFuelOil', 'NaturalGas', 'Uranium', 'BioSolid', 'SolidWaste']
>>> switch_data.data(name='f_co2_intensity')
{'ResidualFuelOil': 0.0788, 'Uranium': 0, 'DistillateFuelOil': 0.07315, 'Coal': 0.09552, 'SolidWaste': 0.095, 'NaturalGas': 0.05306, 'BioSolid': 0.09435}

3/19/2015
I'm thinking about how to implement hybrid facilities to reduce excessive code and related complexities. By hybrid facility, I mean a generator uses multiple energy sources or perhaps separate generating units are linked. The key use cases are:
• Pumped Hydro which includes a storage component and a water-flowing-downhill component
• Compressed Air Energy Storage which includes a storage component and a gas turbine component
Other use cases could be:
• Any combined cycle plant if the gas and steam generators are modeled separately
• Solar thermal with fossil or biomass backup boilers
• Cofiring Coal & biomass
• Oil generators that can run on multiple fuels or need to start on a higher grade fuel before switching to a lower grade fuel

The last three use cases could be modeled as plants that can run on multiple energy sources rather than separate generators with linked investment and operations. 

The status quo for hybrid plants that include a storage plant was to add storage decision variables and attributes, write a number of extra terms and constraints to link their storage and non-storage components, and export their hourly dispatch decisions in two or three rows, for the non-storage component, the store/release of REC eligible energy, and the store/release of non-renewable energy. 

The physical structure of pumped hydro & CAES has a single generator that produced all energy and a pump that stores energy. We tracked ReleaseEnergy separately from DispatchEnergy so that we could track Renewable Energy Credits into and out of storage for modeling renewable portfolio standards. At the time we wrote this, policy hadn't caught up with the idea of tracking renewable energy through storage. As of 2013, some California RPS documentation explicitly addresses this on pages 43-46 and 64-65 of 
http://www.energy.ca.gov/2013publications/CEC-300-2013-005/CEC-300-2013-005-ED7-SD.pdf
This say that storage projects can only count as renewable energy if they are directly connected to a renewable facility and generally owned by the same entity. Any energy that comes out of stand-alone storage projects is not eligible for RECs, even if some renewable energy was stored in it. So, if you are going to store renewable energy in a generic grid storage device, then you should unbundle the REC component before storage to maintain full economic value.

I guess the other reason to track released energy is to ensure energy
balancing in the storage vessel. We could tie this released energy to
the dispatch variable to reflect that it is consumed by the generator to
produce electricity, but it could be difficult to parse out how much of
the dispatch is coming from storage vs the other energy source, at least
for pumped hydro.

4/13

cogen_thermal_demand[prj] was a parameter in SWITCH-WECC used in
assessing the amount of biosolid and biogas fuels consumed by
cogeneration projects. This was fine for the purposes of placing an
upper limit on available fuel, but buggy in assigning the costs of
that fuel to the electricity sector. The units were MMBTU/MWh.  In
all scenarios I am familiar with, Biofuel cogeneration projects
supply a miniscule amount of energy to the grid and are not a
significant part of the results, so I am dropping this parameter for
now.

4/22

While implementing the transmission module, I realized that the way
transmission_end_year is calculated and used for determining capital
repayments is buggy and will tend to over- or under- estimate the total
amount of capital repayments in cases where the lifetime of loan
repayments for a transmission line is not an integer multiple of the
length of an investment period. I think this bug also exists for new
generation projects. At some point I should squash them both at the same
time. The magnitude of this bug will decrease for shorter investment
periods.

6/11

I started rewriting code to use max_capacity_factor for all projects,
and updating documentation to explain the reasoning behind it. In the
process it started bothering me some more.

    prj_max_capacity_factor[prj, t] describes the maximum fraction of
    project capacity that can be dispatched in an hour due to exogenous
    factors wuch as weather. This is most meaningful for variable
    renewable projects such as solar or wind and it is required for
    those types of generators. It can also be used to describe upper
    bounds on hydro dispatch, although absolute units of cubic meters of
    water or MWh dispatch may be more useful for hydro. This is defined
    for every generator type as a mathematical convenience.  For most
    types of generators, this has no clear meaning and should not be
    specified.

The programming pattern of generalizing terms past their semantic bounds
and relying on default data values to ensure mathematically correct
behavior is troubling to me for two reasons. First, the equations lack
clear interpretations in general because key terms are meaningless for
most cases. Second, the mathematical behavior is depend on runtime data
rather than code structure, which means that code reviews cannot ensure
accurate behavior. This basically moves logic to handle particular cases
from code into data. I won't be there at runtime to support most users,
so I would prefer to make this sort of logic explicit in code where I
can review and test it rather than leaving it for the runtime
environment and userland.

To get clear centralized terms for keeping track of bounds, we can do something like this:
    def DispatchUpperLimit_expr(m, pr, t):
        if pr in m.VARIABLE_PROJECTS:
            return m.CommitProject[pr, t] * m.prj_max_capacity_factor[pr, t]
        else:
            return m.CommitProject[pr, t]
    mod.DispatchUpperLimit = Expression(
        mod.PROJ_DISPATCH_POINTS,
        initialize=DispatchUpperLimit_expr)

...I don't know. I went ahead and specified minimum capacity factor as g_min_load_fraction[g] and proj_min_load_fraction[proj, t] in commit.py to get things running. Somehow that doesn't bother me as much as expanding prj_max_capacity_factor[proj, t] to every project. I guess over several years I've gotten used to capacity_factor[project, timepoint] as always meaning the fraction of renewable power a plant can produce in a given timepoint, so stuffing other stuff into there seems to dilute its underlying meaning.

6/18/2015
I was trying to write plenty of documentation for unitcommit.fuel_use, but it's getting overwhelming so I'm going to cut my draft out of there and paste it into here. This allows me to avoid half-written explanations in the module's documentation, but I won't have to start from scratch if I want to revisit that later.

The simplest model of fuel consumption is:
    Fuel = Energy * full_load_heat_rate
This effectively defines an input-output curve that goes from the orgin
(0 energy, 0 fuel) to the maximum power output and fuel consumption rate.
This is often written in units of rates to simplify modeling:
    FuelRate = Power * full_load_heat_rate

If you graph input/output data for actual generators, their lines
typically have a non-zero intercept with the y-axis (FuelRate). The line
touches the original

Also, they can't operate
below some minimum loading level, which makes their input output curve
into a line segment.

have a minimum load restriction that restricts operation on the lower
end of the line. 

Their  new line would starts above 0 on the y-axis and
and touch the first line on the right side at full-load/full fuel
consumption. The slope of the actual line is the incremental heat rate,
which will be less than the full-load heat rate. If you plot both of
these lines on the same graph, you would see that the simple model
underestimates fuel use in all cases except when the plant is running at
full capacity. So an improved model of fuel use will use the equation:
Fuel = Energy * incremental_heat_rate + min_fuel_use. In practice, most
generators have minimum loading levels and can't effectively operate at
very low levels of energy output, but that doesn't change the equation
for fuel use.

For all generator data that I've inspected, the incremental heat rate
tends to increase slightly as energy output is increased. For most
generators this is a small third-order effect that can be neglected for
estimations purposes, but if you want to be completely accurate, you
would draw a series of line segments with increasing slopes. With this
model, you can't use a single linear equation to calculate fuel
requirements because each line segment has a different equations. You
could pick the appropriate equation for each given level of energy
output, but that sort of if-then logic does not translate directly into
linear programming. In linear programming, this situation is dubbed
piecewise linear and there are two basic approaches to handle it.

The first approach is to divide the energy production variable into
several sub- variables (one for each segment), and put an upper bound on
each sub- variable so that it can't exceed the width of the segment. The
total energy production is the sum of the sub-variables, and the total
fuel consumption is: Fuel = min_fuel_use + E0*incremental_heat_rate0 +
E1*incremental_heat_rate1 + ... As long as each incremental_heat_rate is
larger than the one before it, then the optimization will ensure that E1
remains at 0 until E0 is at its upper limit, which ensures consistent
results. This tiered decision method is used in the fuel_markets module.

The second approach is to make Fuel use a second decision variable that
must be above all of the lines. As long as fuel has a cost associated
with it, a cost minimizing optimization will force fuel down to the line
segments. This method also requires that incremental heat rates increase
with energy production so that the lines collectively form a convex
boundary for fuel use.

This module models fuel use as a piecewise linear function where Fuel
becomes a decision variable. The equations are similar to those
described above, except they tend to be in units of FuelUseRate (BTU/h)
and Power (W) rather than Fuel (BTU) and Energy (Wh) because keeping the
duration of power output and fuel use separated makes many parts of
the model easier to express.

In the future, we may write a separate module that models fuel use with
a single line using average incremental heat rate rather than this more
complex piecewise linear formulation. The key equation would be
FuelUseRate = Power * incremental_heat_rate + min_fuel_use. That simpler
model should achieve essentially identical results with less complexity
in most situations, which could provide computational advantages and be
easier to describe. We jumped ahead to the complex version of the model
now because some stakeholders we are working with strongly prefer the
more complicated piecewise linear form, and its easier for us to
implement this than to explain why the simpler form is good enough to
capture over 95 percent of system behaviors.

For more background on incremental vs average heat rates, including an
explanation of how read values from standard tables that are poorly
formatted and misleading, read
http://www.energy.ca.gov/papers/98-04-07_HEATRATE.PDF

6/22/2015 

When implementing support for incremental heat rates, I wanted to
specify individual lines of fuel use rates with an indexed set, where
each member of the set would be a tuple of (intercept, slope). Unfortunately,
Pyomo does not provide the option of a default function to populate indexed
sets.

Over the weekend I thought I would have to rewrite it to a bunch of
parameters instead, but just before I started rewriting it today, I
checked the documentation and found that BuildAction() will work.
